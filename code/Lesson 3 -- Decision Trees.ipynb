{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# magic! (don't worry about this)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let us import some useful things\n",
    "from lib import *\n",
    "from classifiers import *\n",
    "from numpy import mean\n",
    "#from graphs import *\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sally sends you a picture of a dress that she has just found. Excited, you text back \"Did you buy it?\" and she asks you to guess whether she bought the dress or not. You know Sally very well, What is your thought process on whether Sally bought the dress or not? \n",
    "\n",
    "Draw up this process.\n",
    "\n",
    "Lets run an example through this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to make a tree (hand-wavy algorithm):\n",
    "    1. Find the best feature to divide on (this is the hard part)\n",
    "    2. Divide on A and assign training samples to each leaf. \n",
    "    3. If the leaf is \"pure\" stop. Otherwise iterate over\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "$S = - \\sum_i \\log(p_i) p_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armin/miniconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/armin/miniconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in multiply\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "p = np.linspace(0,1,500)\n",
    "y = - np.log(p)*p\n",
    "plt.plot(p,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini index\n",
    "\n",
    "![:(](https://nicfoley.files.wordpress.com/2013/07/genie.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I = \\sum_i p_i (1-p_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.linspace(0,1,500)\n",
    "y = (1-p)*p\n",
    "plt.plot(p,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data same as before\n",
    "microarray_file_name = '../data/leukemia_ALL_AML_matrix.txt'\n",
    "labels_file_name = '../data/leukemia_ALL_AML_labels.txt'\n",
    "data_store = DataSet(microarray_file_name, labels_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This train function is *almost* the same as the train method that \n",
    "# your knn classifier. The difference is that this is a function that\n",
    "# takes as one of the parameters the decision tree, so it will be called\n",
    "# like: train(decision_tree, train_samples)\n",
    "def train(decision_tree, train_samples):\n",
    "    feature_array = [sample.get_gene_profile() for sample in train_samples]\n",
    "    labels = [sample.get_label() for sample in train_samples]\n",
    "    decision_tree.fit(feature_array, labels)\n",
    "\n",
    "def classify(decision_tree, test_samples):\n",
    "    labelled_samples = []\n",
    "    feature_array = [sample.get_gene_profile() for sample in test_samples]\n",
    "    results = decision_tree.predict(feature_array)\n",
    "    labelled_samples = [(test_samples[i], results[i]) for i in range(len(test_samples))]\n",
    "    return labelled_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# Here, you can set criterion = \"entropy\" or \"gini\", which will determine\n",
    "# what equation the decision tree will use to measure the quality of a split\n",
    "#\n",
    "# You can set max_features=None, \"sqrt\", or \"log2\", which will determine how many\n",
    "# features the decision tree will use. Setting it to None will use all the features,\n",
    "# sqrt will use sqrt(number of features) and log2 will use log2(number of features)\n",
    "#\n",
    "# Play around! What settings are best? Do they change for the different data sets?\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_features=None, random_state=408)\n",
    "train(decision_tree, data_store.get_train_set())\n",
    "\n",
    "classified_samples = classify(decision_tree, data_store.get_test_set())\n",
    "# let's evaluate how well the classifier worked\n",
    "evaluate_results(classified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_errors(classified_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bootstrap(data):\n",
    "    N = len(data)\n",
    "    sample = np.random.choice(N, N)\n",
    "    return [data[i] for i in sample]\n",
    "\n",
    "def bagTrain(ntrees):\n",
    "    myTrees = []\n",
    "    for i in range(ntrees):\n",
    "        decision_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_features=None)\n",
    "        train(decision_tree, bootstrap(data_store.get_train_set()))\n",
    "        myTrees.append(decision_tree)\n",
    "    return myTrees\n",
    "\n",
    "def bagClassify(myTrees, test_samples):\n",
    "    classification = []\n",
    "    for sample in test_samples:\n",
    "#         print sample.get_gene_profile()\n",
    "        judgment = []\n",
    "        for tree in myTrees:\n",
    "            judgment.append(classify(tree, [sample]))\n",
    "        judgment = [item[0][1] for item in judgment]\n",
    "        classification.append([sample, mean(judgment) > 0.5])\n",
    "    return classification\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "myTrees = bagTrain(1)\n",
    "classes = bagClassify(myTrees, data_store.get_test_set())\n",
    "evaluate_results(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the regular tree and the bagged tree multiple times. What do you notice? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
